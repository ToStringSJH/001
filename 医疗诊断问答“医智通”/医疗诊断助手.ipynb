{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2676df8c",
   "metadata": {},
   "source": [
    " ## Task 4：源大模型微调实战"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e1881c-f360-4a06-8e0a-abf75524e022",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-25T13:22:30.552735Z",
     "iopub.status.busy": "2024-07-25T13:22:30.552479Z",
     "iopub.status.idle": "2024-07-25T13:22:30.555540Z",
     "shell.execute_reply": "2024-07-25T13:22:30.554971Z",
     "shell.execute_reply.started": "2024-07-25T13:22:30.552716Z"
    },
    "tags": []
   },
   "source": [
    "### 2.1 环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc1e15c7-6663-4060-9e48-10487e10746d",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-25T15:12:45.628538Z",
     "iopub.status.busy": "2024-08-25T15:12:45.628261Z",
     "iopub.status.idle": "2024-08-25T15:12:45.630850Z",
     "shell.execute_reply": "2024-08-25T15:12:45.630396Z",
     "shell.execute_reply.started": "2024-08-25T15:12:45.628520Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 查看已安装依赖\n",
    "# ! pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20b30a21-5618-4379-b10b-fcecc2a8d248",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-25T15:12:45.632066Z",
     "iopub.status.busy": "2024-08-25T15:12:45.631791Z",
     "iopub.status.idle": "2024-08-25T15:12:49.755286Z",
     "shell.execute_reply": "2024-08-25T15:12:49.754777Z",
     "shell.execute_reply.started": "2024-08-25T15:12:45.632051Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: streamlit==1.24.0 in /usr/local/lib/python3.10/site-packages (1.24.0)\n",
      "Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.10/site-packages (from streamlit==1.24.0) (17.0.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/site-packages (from streamlit==1.24.0) (1.8.2)\n",
      "Requirement already satisfied: pandas<3,>=0.25 in /usr/local/lib/python3.10/site-packages (from streamlit==1.24.0) (2.2.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3 in /usr/local/lib/python3.10/site-packages (from streamlit==1.24.0) (3.1.43)\n",
      "Requirement already satisfied: python-dateutil<3,>=2 in /usr/local/lib/python3.10/site-packages (from streamlit==1.24.0) (2.9.0.post0)\n",
      "Requirement already satisfied: rich<14,>=10.11.0 in /usr/local/lib/python3.10/site-packages (from streamlit==1.24.0) (13.7.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.1 in /usr/local/lib/python3.10/site-packages (from streamlit==1.24.0) (4.12.0)\n",
      "Requirement already satisfied: tzlocal<5,>=1.1 in /usr/local/lib/python3.10/site-packages (from streamlit==1.24.0) (4.3.1)\n",
      "Requirement already satisfied: requests<3,>=2.4 in /usr/local/lib/python3.10/site-packages (from streamlit==1.24.0) (2.32.3)\n",
      "Requirement already satisfied: packaging<24,>=14.1 in /usr/local/lib/python3.10/site-packages (from streamlit==1.24.0) (23.2)\n",
      "Requirement already satisfied: importlib-metadata<7,>=1.4 in /usr/local/lib/python3.10/site-packages (from streamlit==1.24.0) (6.11.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/site-packages (from streamlit==1.24.0) (8.1.7)\n",
      "Requirement already satisfied: pympler<2,>=0.9 in /usr/local/lib/python3.10/site-packages (from streamlit==1.24.0) (1.1)\n",
      "Requirement already satisfied: validators<1,>=0.2 in /usr/local/lib/python3.10/site-packages (from streamlit==1.24.0) (0.33.0)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/site-packages (from streamlit==1.24.0) (1.26.3)\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/site-packages (from streamlit==1.24.0) (3.20.3)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/site-packages (from streamlit==1.24.0) (5.4.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/site-packages (from streamlit==1.24.0) (6.4.1)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/site-packages (from streamlit==1.24.0) (5.4.0)\n",
      "Requirement already satisfied: toml<2 in /usr/local/lib/python3.10/site-packages (from streamlit==1.24.0) (0.10.2)\n",
      "Requirement already satisfied: tenacity<9,>=8.0.0 in /usr/local/lib/python3.10/site-packages (from streamlit==1.24.0) (8.5.0)\n",
      "Requirement already satisfied: watchdog in /usr/local/lib/python3.10/site-packages (from streamlit==1.24.0) (4.0.2)\n",
      "Requirement already satisfied: pydeck<1,>=0.1.dev5 in /usr/local/lib/python3.10/site-packages (from streamlit==1.24.0) (0.9.1)\n",
      "Requirement already satisfied: pillow<10,>=6.2.0 in /usr/local/lib/python3.10/site-packages (from streamlit==1.24.0) (9.5.0)\n",
      "Requirement already satisfied: narwhals>=1.1.0 in /usr/local/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit==1.24.0) (1.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit==1.24.0) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit==1.24.0) (4.23.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3->streamlit==1.24.0) (4.0.11)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/site-packages (from importlib-metadata<7,>=1.4->streamlit==1.24.0) (3.19.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas<3,>=0.25->streamlit==1.24.0) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas<3,>=0.25->streamlit==1.24.0) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil<3,>=2->streamlit==1.24.0) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.4->streamlit==1.24.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.4->streamlit==1.24.0) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.4->streamlit==1.24.0) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.4->streamlit==1.24.0) (2.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/site-packages (from rich<14,>=10.11.0->streamlit==1.24.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/site-packages (from rich<14,>=10.11.0->streamlit==1.24.0) (2.18.0)\n",
      "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.10/site-packages (from tzlocal<5,>=1.1->streamlit==1.24.0) (0.1.0.post0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3->streamlit==1.24.0) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->altair<6,>=4.0->streamlit==1.24.0) (2.1.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.24.0) (23.2.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.24.0) (0.19.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.24.0) (0.35.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.24.0) (2023.12.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.11.0->streamlit==1.24.0) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 安装 streamlit\n",
    "!pip install streamlit==1.24.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e959377",
   "metadata": {},
   "source": [
    "### 2.2 模型下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb2cf3e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T15:12:49.756328Z",
     "iopub.status.busy": "2024-08-25T15:12:49.756145Z",
     "iopub.status.idle": "2024-08-25T15:12:52.016959Z",
     "shell.execute_reply": "2024-08-25T15:12:52.016437Z",
     "shell.execute_reply.started": "2024-08-25T15:12:49.756311Z"
    }
   },
   "outputs": [],
   "source": [
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('IEITYuan/Yuan2-2B-Mars-hf', cache_dir='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971eb160",
   "metadata": {},
   "source": [
    "### 2.3 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f62ec38",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-25T15:12:52.018102Z",
     "iopub.status.busy": "2024-08-25T15:12:52.017634Z",
     "iopub.status.idle": "2024-08-25T15:12:57.448111Z",
     "shell.execute_reply": "2024-08-25T15:12:57.447605Z",
     "shell.execute_reply.started": "2024-08-25T15:12:52.018083Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-25 23:12:52.470066: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-25 23:12:52.481427: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-25 23:12:52.494895: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-25 23:12:52.498957: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-25 23:12:52.508999: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-25 23:12:53.267656: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/local/lib/python3.10/site-packages/_distutils_hack/__init__.py:55: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 导入环境\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "from modelscope.msdatasets import MsDataset\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e098d9eb",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-25T15:12:57.450074Z",
     "iopub.status.busy": "2024-08-25T15:12:57.449504Z",
     "iopub.status.idle": "2024-08-25T15:12:57.470043Z",
     "shell.execute_reply": "2024-08-25T15:12:57.469597Z",
     "shell.execute_reply.started": "2024-08-25T15:12:57.450055Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "df = pd.read_json('./data1.json')\n",
    "ds = Dataset.from_pandas(df)\n",
    "# 加载 MsDataset 数据集\n",
    "# train_datasets = MsDataset.load('multico_ner_2023_wiki128', subset_name='zh', namespace='pangda', split='train')\n",
    "# dev_datasets = MsDataset.load('multico_ner_2023_wiki128', subset_name='zh', namespace='pangda', split='dev')\n",
    "# test_datasets = MsDataset.load('multico_ner_2023_wiki128', subset_name='zh', namespace='pangda', split='test')\n",
    "\n",
    "# # 提取 MsDataset 中的数据\n",
    "# def extract_data(msdataset):\n",
    "#     data = []\n",
    "#     for sample in msdataset:\n",
    "#         data.append(sample)\n",
    "#     return data\n",
    "\n",
    "# # 将提取的数据转换为 pandas DataFrame\n",
    "# train_data = extract_data(train_datasets)\n",
    "# dev_data = extract_data(dev_datasets)\n",
    "# test_data = extract_data(test_datasets)\n",
    "\n",
    "# train_df = pd.DataFrame(train_data)\n",
    "# dev_df = pd.DataFrame(dev_data)\n",
    "# test_df = pd.DataFrame(test_data)\n",
    "\n",
    "# # 将 pandas DataFrame 转换为 datasets.Dataset\n",
    "# train_ds = Dataset.from_pandas(train_df)\n",
    "# dev_ds = Dataset.from_pandas(dev_df)\n",
    "# test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "# # 选择一个数据集用于示例（例如 train_ds）\n",
    "# ds = dev_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72daff8f-b82f-495c-9127-f5608cb41db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ac92d42-efae-49b1-a00e-ccaa75b98938",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-25T15:12:57.470968Z",
     "iopub.status.busy": "2024-08-25T15:12:57.470787Z",
     "iopub.status.idle": "2024-08-25T15:12:57.476065Z",
     "shell.execute_reply": "2024-08-25T15:12:57.475597Z",
     "shell.execute_reply.started": "2024-08-25T15:12:57.470952Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': ['现在18岁了，最近半年，发觉，性生活总是提不起劲，同时，每次才开始就已经射了，请问：男孩早泄究竟是什么因素引发的。'],\n",
       " 'output': ['可能诱发早泄的因素，与多个方面都有一定的影响，常见的是患者自身的心里因素，在性生活时受到一些不良因素的影响，或者是夫妻感情不合，也可能是因为患者自身缺少性知识，常常手淫就会引起早泄，除了这两点，男性如果患上一些男科疾病也会诱发早泄，比如像前列腺炎、前列腺钙化等，或是骨盆骨折以及高血糖等，早泄对男性身心造成的创伤都是较大的，建议了解病因后正确预防。']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看数据\n",
    "len(ds)\n",
    "ds[:1]\n",
    "# 查看数据集中一个样本的字段名\n",
    "# print(train_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74ee5a67-2e55-4974-b90e-cbf492de500a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T15:12:57.476869Z",
     "iopub.status.busy": "2024-08-25T15:12:57.476658Z",
     "iopub.status.idle": "2024-08-25T15:12:58.878890Z",
     "shell.execute_reply": "2024-08-25T15:12:58.878389Z",
     "shell.execute_reply.started": "2024-08-25T15:12:57.476853Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "# 加载 tokenizer\n",
    "path = './IEITYuan/Yuan2-2B-Mars-hf'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')\n",
    "tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>','<commit_before>','<commit_msg>','<commit_after>','<jupyter_start>','<jupyter_text>','<jupyter_code>','<jupyter_output>','<empty_output>'], special_tokens=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2503a5fa-9621-4495-9035-8e7ef6525691",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-25T15:12:58.879836Z",
     "iopub.status.busy": "2024-08-25T15:12:58.879599Z",
     "iopub.status.idle": "2024-08-25T15:12:58.884784Z",
     "shell.execute_reply": "2024-08-25T15:12:58.884380Z",
     "shell.execute_reply.started": "2024-08-25T15:12:58.879808Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义数据处理函数\n",
    "def process_func(example):\n",
    "    MAX_LENGTH = 384    # Llama分词器会将一个中文字切分为多个token，因此需要放开一些最大长度，保证数据的完整性\n",
    "\n",
    "    instruction = tokenizer(f\"{example['input']}<sep>\")\n",
    "    response = tokenizer(f\"{example['output']}<eod>\")\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"]\n",
    "    attention_mask = [1] * len(input_ids) \n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] # instruction 不计算loss\n",
    "\n",
    "    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# def process_func(example):\n",
    "#     MAX_LENGTH = 384    # 设定最大长度\n",
    "\n",
    "#     # 将 tokens 拼接为字符串\n",
    "#     tokens = example['tokens']\n",
    "#     spans = example['spans']\n",
    "    \n",
    "#     # 将 tokens 转换为模型输入 ID\n",
    "#     encodings = tokenizer(tokens, is_split_into_words=True, truncation=True, padding='max_length', max_length=MAX_LENGTH)\n",
    "#     input_ids = encodings[\"input_ids\"]\n",
    "#     attention_mask = encodings[\"attention_mask\"]\n",
    "    \n",
    "#     # 创建标签数组\n",
    "#     labels = [-100] * len(input_ids)  # 初始化所有标签为 -100\n",
    "    \n",
    "#     # 将 spans 中的实体标签对齐到 token ID 上\n",
    "#     for span in spans:\n",
    "#         start, end = span['start'], span['end']\n",
    "#         label = span['type']\n",
    "#         # 在这里处理对齐标签的逻辑\n",
    "#         # 请根据实际情况对齐标签，下面的示例是简化版本\n",
    "#         for idx in range(start, end + 1):\n",
    "#             if idx < len(labels):  # 防止索引超出范围\n",
    "#                 labels[idx] = label_to_id(label)  # 将实体标签转换为 ID\n",
    "    \n",
    "#     return {\n",
    "#         \"input_ids\": input_ids,\n",
    "#         \"attention_mask\": attention_mask,\n",
    "#         \"labels\": labels\n",
    "#     }\n",
    "\n",
    "# def label_to_id(label):\n",
    "#     # 假设有一个标签到 ID 的映射字典\n",
    "#     label2id = {\"Politician\": 1, \"Location\": 2, \"Organization\": 3, \"Date\": 4}\n",
    "#     return label2id.get(label, -100)  # 默认标签 -100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84f870d6-73a9-4b0f-8abf-687b32224ad8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T15:12:58.885749Z",
     "iopub.status.busy": "2024-08-25T15:12:58.885489Z",
     "iopub.status.idle": "2024-08-25T15:12:59.228806Z",
     "shell.execute_reply": "2024-08-25T15:12:59.228331Z",
     "shell.execute_reply.started": "2024-08-25T15:12:58.885733Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5dbbb1043948d9a52f29e6d4470fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/828 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 828\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 处理数据集\n",
    "tokenized_id = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f7e15a0-4d9a-4935-9861-00cc472654b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T15:12:59.229736Z",
     "iopub.status.busy": "2024-08-25T15:12:59.229488Z",
     "iopub.status.idle": "2024-08-25T15:12:59.234712Z",
     "shell.execute_reply": "2024-08-25T15:12:59.234323Z",
     "shell.execute_reply.started": "2024-08-25T15:12:59.229719Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'现在18岁了，最近半年，发觉，性生活总是提不起劲，同时，每次才开始就已经射了，请问：男孩早泄究竟是什么因素引发的。<sep> 可能诱发早泄的因素，与多个方面都有一定的影响，常见的是患者自身的心里因素，在性生活时受到一些不良因素的影响，或者是夫妻感情不合，也可能是因为患者自身缺少性知识，常常手淫就会引起早泄，除了这两点，男性如果患上一些男科疾病也会诱发早泄，比如像前列腺炎、前列腺钙化等，或是骨盆骨折以及高血糖等，早泄对男性身心造成的创伤都是较大的，建议了解病因后正确预防。<eod>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据检查\n",
    "tokenizer.decode(tokenized_id[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97f16f66-324a-454f-8cc3-ef23b100ecff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T15:12:59.235512Z",
     "iopub.status.busy": "2024-08-25T15:12:59.235286Z",
     "iopub.status.idle": "2024-08-25T15:12:59.239703Z",
     "shell.execute_reply": "2024-08-25T15:12:59.239324Z",
     "shell.execute_reply.started": "2024-08-25T15:12:59.235497Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'可能诱发早泄的因素，与多个方面都有一定的影响，常见的是患者自身的心里因素，在性生活时受到一些不良因素的影响，或者是夫妻感情不合，也可能是因为患者自身缺少性知识，常常手淫就会引起早泄，除了这两点，男性如果患上一些男科疾病也会诱发早泄，比如像前列腺炎、前列腺钙化等，或是骨盆骨折以及高血糖等，早泄对男性身心造成的创伤都是较大的，建议了解病因后正确预防。<eod>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(list(filter(lambda x: x != -100, tokenized_id[0][\"labels\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9127a0d",
   "metadata": {},
   "source": [
    "### 2.4 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "170764e5-d899-4ef4-8c53-36f6dec0d198",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T15:12:59.240481Z",
     "iopub.status.busy": "2024-08-25T15:12:59.240257Z",
     "iopub.status.idle": "2024-08-25T15:13:02.082344Z",
     "shell.execute_reply": "2024-08-25T15:13:02.081894Z",
     "shell.execute_reply.started": "2024-08-25T15:12:59.240467Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YuanForCausalLM(\n",
       "  (model): YuanModel(\n",
       "    (embed_tokens): Embedding(135040, 2048, padding_idx=77185)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x YuanDecoderLayer(\n",
       "        (self_attn): YuanAttention(\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): YuanRotaryEmbedding()\n",
       "          (lf_gate): LocalizedFiltering(\n",
       "            (conv1): Conv2d(2048, 1024, kernel_size=(2, 1), stride=(1, 1), padding=(1, 0))\n",
       "            (conv2): Conv2d(1024, 2048, kernel_size=(2, 1), stride=(1, 1), padding=(1, 0))\n",
       "            (output_layernorm): YuanRMSNorm()\n",
       "          )\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): YuanMLP(\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): YuanRMSNorm()\n",
       "        (post_attention_layernorm): YuanRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): YuanRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=135040, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型加载\n",
    "model = AutoModelForCausalLM.from_pretrained(path, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2323eac7-37d5-4288-8bc5-79fac7113402",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T15:13:02.083424Z",
     "iopub.status.busy": "2024-08-25T15:13:02.083018Z",
     "iopub.status.idle": "2024-08-25T15:13:02.086298Z",
     "shell.execute_reply": "2024-08-25T15:13:02.085893Z",
     "shell.execute_reply.started": "2024-08-25T15:13:02.083408Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.enable_input_require_grads() # 开启gradient_checkpointing时，要执行该方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f808b05c-f2cb-48cf-a80d-0c42be6051c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T15:13:02.088552Z",
     "iopub.status.busy": "2024-08-25T15:13:02.088248Z",
     "iopub.status.idle": "2024-08-25T15:13:02.091296Z",
     "shell.execute_reply": "2024-08-25T15:13:02.090894Z",
     "shell.execute_reply.started": "2024-08-25T15:13:02.088537Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看模型数据类型\n",
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d304ae2-ab60-4080-a80d-19cac2e3ade3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T15:13:02.092141Z",
     "iopub.status.busy": "2024-08-25T15:13:02.091817Z",
     "iopub.status.idle": "2024-08-25T15:13:02.096351Z",
     "shell.execute_reply": "2024-08-25T15:13:02.095887Z",
     "shell.execute_reply.started": "2024-08-25T15:13:02.092126Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules={'k_proj', 'q_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'}, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 配置Lora\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False, # 训练模式\n",
    "    r=8, # Lora 秩\n",
    "    lora_alpha=32, # Lora alaph，具体作用参见 Lora 原理\n",
    "    lora_dropout=0.1# Dropout 比例\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c2489c5-eaab-4e1f-b06a-c3f914b4bf8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T15:13:02.097270Z",
     "iopub.status.busy": "2024-08-25T15:13:02.096977Z",
     "iopub.status.idle": "2024-08-25T15:13:02.398373Z",
     "shell.execute_reply": "2024-08-25T15:13:02.397851Z",
     "shell.execute_reply.started": "2024-08-25T15:13:02.097255Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): YuanForCausalLM(\n",
       "      (model): YuanModel(\n",
       "        (embed_tokens): Embedding(135040, 2048, padding_idx=77185)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x YuanDecoderLayer(\n",
       "            (self_attn): YuanAttention(\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): YuanRotaryEmbedding()\n",
       "              (lf_gate): LocalizedFiltering(\n",
       "                (conv1): Conv2d(2048, 1024, kernel_size=(2, 1), stride=(1, 1), padding=(1, 0))\n",
       "                (conv2): Conv2d(1024, 2048, kernel_size=(2, 1), stride=(1, 1), padding=(1, 0))\n",
       "                (output_layernorm): YuanRMSNorm()\n",
       "              )\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): YuanMLP(\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): YuanRMSNorm()\n",
       "            (post_attention_layernorm): YuanRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): YuanRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=135040, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建PeftModel\n",
    "model = get_peft_model(model, config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebf5482b-fab9-4eb3-ad88-c116def4be12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T15:13:02.399335Z",
     "iopub.status.busy": "2024-08-25T15:13:02.399083Z",
     "iopub.status.idle": "2024-08-25T15:13:02.405299Z",
     "shell.execute_reply": "2024-08-25T15:13:02.404845Z",
     "shell.execute_reply.started": "2024-08-25T15:13:02.399318Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,043,968 || all params: 2,097,768,448 || trainable%: 0.4311\n"
     ]
    }
   ],
   "source": [
    "# 打印需要训练的参数\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e76bbff-15fd-4995-a61d-8364dc5e9ea0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-25T15:13:02.406272Z",
     "iopub.status.busy": "2024-08-25T15:13:02.406119Z",
     "iopub.status.idle": "2024-08-25T15:13:02.419297Z",
     "shell.execute_reply": "2024-08-25T15:13:02.418843Z",
     "shell.execute_reply.started": "2024-08-25T15:13:02.406257Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 设置训练参数\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./output/Yuan2.0-2B_lora_bf16\",\n",
    "    per_device_train_batch_size=12,\n",
    "    gradient_accumulation_steps=1,\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=True,\n",
    "    bf16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f142cb9c-ad99-48e6-ba86-6df198f9ed96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T15:13:02.420211Z",
     "iopub.status.busy": "2024-08-25T15:13:02.419931Z",
     "iopub.status.idle": "2024-08-25T15:13:02.454304Z",
     "shell.execute_reply": "2024-08-25T15:13:02.453840Z",
     "shell.execute_reply.started": "2024-08-25T15:13:02.420196Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.19.24, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# 初始化Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_id,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aec9bc36-b297-45af-99e1-d4c4d82be081",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T15:13:02.455155Z",
     "iopub.status.busy": "2024-08-25T15:13:02.454927Z",
     "iopub.status.idle": "2024-08-25T15:16:49.222784Z",
     "shell.execute_reply": "2024-08-25T15:16:49.222321Z",
     "shell.execute_reply.started": "2024-08-25T15:13:02.455140Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-25 23:13:02,706] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='207' max='207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [207/207 03:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.835100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.671700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.451400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.973200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.637200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.488100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.944700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.145900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.889700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.702600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.501800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.415700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.340300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.536900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.673300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.680400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.565700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.451200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.449700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.375500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.492500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.236900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.598900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.349900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.135700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.058700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.236300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.491100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.012900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.366000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.190500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.951200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.789000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.769300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.871100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.595700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.100800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>3.048500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.448300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2.379100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.531800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.504700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.810800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>2.525800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>2.582400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>2.484700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.651700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.478200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.407100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.635600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.369800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.892100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.617300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>3.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.730200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>2.525400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.681500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>2.372700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>2.284500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.634500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>2.399000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.821500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>2.033700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.832000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>2.013200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.811100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.841200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>2.162700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>2.606000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>2.022600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.991100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.050100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>2.106900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>2.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.947000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.087500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.649100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.205900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.762800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.861200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.889600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.717600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.830100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.284600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.363000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.820100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.754400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.986500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>2.854800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.614200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.984900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.902300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.593400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.213800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>2.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.222300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.192100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.552700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>1.386100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.087100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.551100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>1.780500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.979200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>1.082800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>2.090700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.331900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>2.032300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.765600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>1.661500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>1.907900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.979800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>1.206300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>1.107000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>1.933200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>2.041800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.223100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>2.241400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.694900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>1.730900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>1.815800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.401100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>1.775700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>1.810800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>1.154800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>1.805700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.240100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>1.756100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>1.748800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>2.091100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>1.662600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.778400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>1.674300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>1.371800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>1.913200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.218400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>1.233400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>1.562700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>2.035200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>2.408900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.954100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.794900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>1.060600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>1.260500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>1.588300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.434900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>1.820600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>1.390100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>1.922400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>1.490200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1.916800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>2.318600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.745900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>1.484200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>1.030300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.918100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>1.133200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>1.746200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.787100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>1.507400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>1.089900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>1.117900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>1.510900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>1.147500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.665800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>1.170400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>1.527300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>1.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.911800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.776800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>1.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>2.107600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>1.555200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>1.075700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.257400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>1.643900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>1.138800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>2.374600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>1.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>1.415800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>1.489500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.888500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>1.078100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.982600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.311000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>1.480600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>2.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>1.617500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>1.228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>1.133300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>1.188900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>1.346400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>1.029300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>1.807600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.734700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>1.597700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>1.042100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>1.544500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>1.684100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>1.299900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>1.423900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>1.378400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./IEITYuan/Yuan2-2B-Mars-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./IEITYuan/Yuan2-2B-Mars-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./IEITYuan/Yuan2-2B-Mars-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./IEITYuan/Yuan2-2B-Mars-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=207, training_loss=2.058268141631343, metrics={'train_runtime': 226.2435, 'train_samples_per_second': 10.979, 'train_steps_per_second': 0.915, 'total_flos': 5028584338243584.0, 'train_loss': 2.058268141631343, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abb2327-458e-4e96-ac98-2141b5b97c8e",
   "metadata": {},
   "source": [
    "### 2.5 效果验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd2a415a-a9ad-49ea-877f-243558a83bfc",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-25T15:18:24.962639Z",
     "iopub.status.busy": "2024-08-25T15:18:24.962319Z",
     "iopub.status.idle": "2024-08-25T15:18:24.966289Z",
     "shell.execute_reply": "2024-08-25T15:18:24.965762Z",
     "shell.execute_reply.started": "2024-08-25T15:18:24.962621Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义生成函数\n",
    "def generate(prompt):\n",
    "    prompt = prompt + \"<sep>\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
    "    outputs = model.generate(inputs, do_sample=False, max_length=812)\n",
    "    output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(output.split(\"<sep>\")[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bd3e756",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-25T15:19:37.120240Z",
     "iopub.status.busy": "2024-08-25T15:19:37.119899Z",
     "iopub.status.idle": "2024-08-25T15:19:37.124401Z",
     "shell.execute_reply": "2024-08-25T15:19:37.123789Z",
     "shell.execute_reply.started": "2024-08-25T15:19:37.120222Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 输入prompt template\n",
    "# template = '''\n",
    "# # 任务描述\n",
    "# 假设你是一个AI助手，能从简历中识别出所有的命名实体，并以json格式返回结果。\n",
    "\n",
    "# # 任务要求\n",
    "# 实体的类别包括：姓名、国籍、种族、职位、教育背景、专业、组织名、地名。\n",
    "# 返回的json格式是一个字典，其中每个键是实体的类别，值是一个列表，包含实体的文本。\n",
    "\n",
    "# # 样例\n",
    "# 输入：\n",
    "# 张三，男，中国籍，工程师\n",
    "# 输出：\n",
    "# {\"姓名\": [\"张三\"], \"国籍\": [\"中国\"], \"职位\": [\"工程师\"]}\n",
    "\n",
    "# # 当前简历\n",
    "# input_str\n",
    "\n",
    "# # 任务重述\n",
    "# 请参考样例，按照任务要求，识别出当前简历中所有的命名实体，并以json格式返回结果。\n",
    "# '''\n",
    "\n",
    "# 输入prompt template\n",
    "# template = '''\n",
    "# # 任务描述\n",
    "# 假设你是一个AI助手，能够从给定文本中识别出所有的命名实体，并以JSON格式返回结果。\n",
    "\n",
    "# # 任务要求\n",
    "# 实体的类别包括：姓名、组织名、地名、职位、日期、其他。\n",
    "# 返回的JSON格式是一个字典，其中每个键是实体的类别，值是一个列表，包含该类别下的所有实体文本。\n",
    "\n",
    "# # 样例\n",
    "# 输入：\n",
    "# 张三是北京大学的教授。李四于2024年在上海获得了硕士学位。\n",
    "# 输出：\n",
    "# {\"姓名\": [\"张三\", \"李四\"], \"地名\": [\"北京\", \"上海\"], \"职位\": [\"教授\"], \"日期\": [\"2024年\"]}\n",
    "\n",
    "# # 当前文本\n",
    "# input_str\n",
    "\n",
    "# # 任务重述\n",
    "# 请参考样例，按照任务要求，识别出当前文本中所有的命名实体，并以JSON格式返回结果。\n",
    "# '''\n",
    "# # 任务描述\n",
    "# 假设你是一个AI医疗诊断助手，能够根据用户提供的症状描述给出相应的医疗建议。\n",
    "\n",
    "# # 任务要求\n",
    "# 返回的医疗建议应该包括诊断、治疗建议、可能的原因以及预防措施。# 样例\n",
    "# 输入：\n",
    "# 男宝，刚满3个月，一开始，说话时觉得嗓子疼，发现，咳嗽好像也比较严重，并且，一直都有点发烧，请问：小孩扁桃体炎难受怎样治效果才好?\n",
    "# 输出：\n",
    "# 在治疗上的话一般建议遵医嘱给孩子吃点消炎的药物，除全身用药的话也可进行局部治疗，比如扁桃体隐窝冲洗，或是扁桃体内药物注射等，必要的时候，比如孩子咽痛尤甚的时候，也是可以给点止痛药的，如果孩子还有点发烧的话那么就应该酌情进行退烧，要是高于40摄氏度的话，那么就需要尽早就医了，除了这些药物治疗的方法之外，如果孩子总是反复发生炎症的话，家长也可在医生的见一下选择手术进行治疗的，期间注意让孩子充分休息，并且多吃一些蔬菜水果。\n",
    "\n",
    "template = '''\n",
    "\n",
    "\n",
    "\n",
    "# 当前文本\n",
    "input_str\n",
    "\n",
    "# 任务重述\n",
    "请参考样例，按照任务要求，根据当前文本中的症状描述给出医疗建议。\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f88a87c6-e330-430b-bd24-69f5180034c2",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-08-25T15:19:39.454664Z",
     "iopub.status.busy": "2024-08-25T15:19:39.454309Z",
     "iopub.status.idle": "2024-08-25T15:19:45.744012Z",
     "shell.execute_reply": "2024-08-25T15:19:45.743442Z",
     "shell.execute_reply.started": "2024-08-25T15:19:39.454644Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 当前文本\n",
      "女宝宝，才1个月，一开始，说嗓子喝水都觉得疼，发现，吞咽好像有点困难，并且，好像低烧一直都没退，请问：小孩扁桃体炎难受怎样治疗?\n",
      "\n",
      "# 任务重述\n",
      "请参考样例，按照任务要求，根据当前文本中的症状描述给出医疗建议。 根据你的描述，孩子扁桃体炎的症状主要是咽痛、发烧、咳嗽，并且孩子还伴发疼痛的症状，建议家长是可以酌情给点镇痛药缓解的，如果孩子还有点发烧的话那么就应该酌情进行退烧，如果高于40摄氏度的话，那么就需要尽早就医了，扁桃体炎是具有反复发作的特征的，如果存在类似情况，炎症得到控制后可以考虑手术，治疗期间还要注意保持饮食卫生，以及足够的休息时间。\n"
     ]
    }
   ],
   "source": [
    "input_str = '女宝宝，才1个月，一开始，说嗓子喝水都觉得疼，发现，吞咽好像有点困难，并且，好像低烧一直都没退，请问：小孩扁桃体炎难受怎样治疗?'\n",
    "prompt = template.replace('input_str', input_str).strip()\n",
    "generate(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "share": {
   "datetime": "2024-08-19T09:23:26.111Z",
   "image": {
    "name": "modelscope:1.17.1-pytorch2.3.0tensorflow2.16.1-gpu-py310-cu121-ubuntu22.04",
    "url": "dsw-registry-vpc.cn-hangzhou.cr.aliyuncs.com/pai/modelscope:1.17.1-pytorch2.3.0tensorflow2.16.1-gpu-py310-cu121-ubuntu22.04"
   },
   "instance": "dsw-d0866eb9f6e76bca",
   "spec": {
    "id": "ecs.gn7i-c8g1.2xlarge",
    "type": "GPU"
   },
   "uid": "1551923101177407"
  },
  "vscode": {
   "interpreter": {
    "hash": "c6bd51bef5b4ff0d27ffa8191406cdd00b92260c116fd6161fe29291443fd9f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
