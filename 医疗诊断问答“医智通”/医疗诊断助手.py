from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import streamlit as st
from peft import PeftModel
import pandas as pd

# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªå‰¯æ ‡é¢˜
st.title("ğŸ’¬ Yuan2.0 åŒ»ç–—è¯Šæ–­é—®ç­”â€œåŒ»æ™ºé€šâ€")

# æºå¤§æ¨¡å‹ä¸‹è½½
from modelscope import snapshot_download
model_dir = snapshot_download('IEITYuan/Yuan2-2B-Mars-hf', cache_dir='./')

# å®šä¹‰æ¨¡å‹è·¯å¾„
path = './IEITYuan/Yuan2-2B-Mars-hf'
lora_path = './output/Yuan2.0-2B_lora_bf16/checkpoint-51'

# å®šä¹‰æ¨¡å‹æ•°æ®ç±»å‹
torch_dtype = torch.bfloat16  # A10
# torch_dtype = torch.float16  # P100

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–æ¨¡å‹å’Œtokenizer
@st.cache_resource
def get_model():
    print("Creat tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')
    tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>',
                         '<commit_before>', '<commit_msg>', '<commit_after>', '<jupyter_start>', '<jupyter_text>',
                         '<jupyter_code>', '<jupyter_output>', '<empty_output>'], special_tokens=True)

    print("Creat model...")
    model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch_dtype, trust_remote_code=True).cuda()
    model = PeftModel.from_pretrained(model, model_id=lora_path)

    return tokenizer, model

# åŠ è½½modelå’Œtokenizer
tokenizer, model = get_model()

template = '''
# ä»»åŠ¡æè¿°
å‡è®¾ä½ æ˜¯ä¸€ä¸ªAIåŒ»ç–—è¯Šæ–­åŠ©æ‰‹ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·æä¾›çš„ç—‡çŠ¶æè¿°ç»™å‡ºç›¸åº”çš„åŒ»ç–—å»ºè®®ã€‚

# ä»»åŠ¡è¦æ±‚
è¿”å›çš„åŒ»ç–—å»ºè®®åº”è¯¥åŒ…æ‹¬è¯Šæ–­ã€æ²»ç–—å»ºè®®ã€å¯èƒ½çš„åŸå› ä»¥åŠé¢„é˜²æªæ–½ã€‚

# æ ·ä¾‹
è¾“å…¥ï¼š
æˆ‘å®¶çš„å­©å­æ˜¯ç”·å®å®ï¼Œ9å²ï¼Œåˆšå¼€å§‹ï¼Œå—“å­çœ¼æœ‰ç‚¹ç—›
è¾“å‡ºï¼š
å¦‚æœå­©å­å¾—äº†æ‰æ¡ƒä½“ç‚çš„è¯ï¼Œé¦–å…ˆå¯ä»¥ç”¨ç‚¹å¯¹ç—‡çš„æŠ—ç”Ÿç´ è¯ç‰©ï¼Œä¹Ÿå¯å±€éƒ¨å†²æ´—æˆ–æ˜¯å±€éƒ¨å–·è¯ï¼Œæ‰æ¡ƒä½“å†…ä¹Ÿå¯æ³¨å°„å¯¹ç—‡è¯ç‰©ï¼Œç–—æ•ˆéƒ½æ˜¯ä¸é”™çš„ï¼Œç—‡çŠ¶å¦‚æœæ˜¯ä»¥å’½ç—›ä¸ºä¸»çš„ï¼Œå¯è€ƒè™‘ç»™ç‚¹é•‡ç—›ç±»çš„è¯ç‰©ï¼Œå¦å¤–å¦‚æœä¼´æœ‰å‘çƒ§çš„æƒ…å†µçš„è¯ï¼Œé‚£ä¹ˆä¹Ÿå¯æœç”¨ä¸€äº›é€€çƒ§è¯ï¼Œé«˜çƒ§çš„è¯è¿˜æ˜¯å»ºè®®è¦å°½æ—©å°±åŒ»çš„ï¼Œå‡ºç°å¤šæ¬¡æ€¥æ€§ä¸¥é‡ï¼Œæˆ–æ˜¯å·²æœ‰å¹¶å‘ç—‡çš„ï¼Œå»ºè®®æ€¥æ€§ç‚ç—‡æ¶ˆé€€äºŒå‘¨åæ–½è¡Œæ‰æ¡ƒä½“åˆ‡é™¤æœ¯ï¼Œå®¶é•¿å¹³æ—¶è¿˜è¦æ³¨æ„ç»™å­©å­åšå¥½ä¿æš–å·¥ä½œï¼Œä»¥å…å—å‡‰æ„Ÿå†’è¯±å‘æ‰æ¡ƒä½“çš„å†æ¬¡å‘ç‚ã€‚

# å½“å‰æ–‡æœ¬
input_str

# ä»»åŠ¡é‡è¿°
è¯·å‚è€ƒæ ·ä¾‹ï¼ŒæŒ‰ç…§ä»»åŠ¡è¦æ±‚ï¼Œæ ¹æ®å½“å‰æ–‡æœ¬ä¸­çš„ç—‡çŠ¶æè¿°ç»™å‡ºåŒ»ç–—å»ºè®®ã€‚
'''

# åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
st.chat_message("assistant").write(f"è¯·è¾“å…¥ç—‡çŠ¶æè¿°ï¼š")

# å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å†…å®¹ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
if query := st.chat_input():
    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
    st.chat_message("user").write(query)

    # è°ƒç”¨æ¨¡å‹
    prompt = template.replace('input_str', query).strip()
    prompt += "<sep>"
    inputs = tokenizer(prompt, return_tensors="pt")["input_ids"].cuda()
    outputs = model.generate(inputs, do_sample=False, max_length=8192)  # è®¾ç½®è§£ç æ–¹å¼å’Œæœ€å¤§ç”Ÿæˆé•¿åº¦
    output = tokenizer.decode(outputs[0])
    response = output.split("<sep>")[-1].replace("<eod>", '').strip()

    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
    st.chat_message("assistant").write(f"æ­£åœ¨ç”ŸæˆåŒ»ç–—å»ºè®®ï¼Œè¯·ç¨å€™...")

    # æ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
    st.chat_message("assistant").write(response)
